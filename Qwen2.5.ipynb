{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Qwen2.5 Training & Inference Setup (LlamaPIE Adaptation)\n",
				"\n",
				"This notebook adapts the LlamaPIE pipeline to train and run inference using **Qwen2.5** models.\n",
				"\n",
				"**Models:**\n",
				"- Small (Classifier): `Qwen/Qwen2.5-1.5B-Instruct`\n",
				"- Large (Generator): `Qwen/Qwen2.5-7B-Instruct`\n",
				"\n",
				"**Steps:**\n",
				"1. Setup Environment\n",
				"2. Patch/Create Code for Qwen Support\n",
				"3. Train Small Model (Classifier)\n",
				"4. Train Large Model (Generator)\n",
				"5. Run Inference"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!nvidia-smi"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# 1. Setup Environment\n",
				"!git clone https://github.com/YudoongY/LlamaPIE.git\n",
				"%cd LlamaPIE\n",
				"!sed -i 's/trainer==0.0.36/# trainer==0.0.36/' requirements.txt\n",
				"!pip install -q -r requirements.txt\n",
				"!pip install -q huggingface_hub accelerate"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from huggingface_hub import login\n",
				"# Login with your token\n",
				"login(token='YOUR_TOKEN_HERE')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Download Data\n",
				"!gdown 1TEquHZR8E53WLR-v09F1Do5UMQH5tjYZ -O Llamapie_dataset.tar.gz\n",
				"!tar -xf Llamapie_dataset.tar.gz"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. Code Adaptation for Qwen"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%writefile model/CasualTokenClassificationQwen.py\n",
				"from transformers.models.qwen2.modeling_qwen2 import *\n",
				"from transformers.modeling_outputs import CausalLMOutputWithPast\n",
				"from transformers.cache_utils import Cache\n",
				"from transformers.utils import (\n",
				"    add_start_docstrings_to_model_forward,\n",
				"    replace_return_docstrings,\n",
				")\n",
				"\n",
				"QWEN2_INPUTS_DOCSTRING = \"\"\n",
				"import math\n",
				"from typing import List, Optional, Tuple, Union\n",
				"\n",
				"import torch\n",
				"import torch.nn.functional as F\n",
				"import torch.utils.checkpoint\n",
				"from torch import nn\n",
				"from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
				"\n",
				"_CONFIG_FOR_DOC = \"Qwen2Config\"\n",
				"\n",
				"class Qwen2ForCausalLM_TokenClassification(Qwen2PreTrainedModel):\n",
				"    _tied_weights_keys = [\"lm_head.weight\"]\n",
				"\n",
				"    def __init__(self, config):\n",
				"        super().__init__(config)\n",
				"        self.model = Qwen2Model(config)\n",
				"        self.num_labels = config.num_labels\n",
				"        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
				"        self.classifier = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
				"        self.post_init()\n",
				"\n",
				"    def get_input_embeddings(self):\n",
				"        return self.model.embed_tokens\n",
				"\n",
				"    def set_input_embeddings(self, value):\n",
				"        self.model.embed_tokens = value\n",
				"\n",
				"    def get_output_embeddings(self):\n",
				"        return self.lm_head\n",
				"\n",
				"    def set_output_embeddings(self, new_embeddings):\n",
				"        self.lm_head = new_embeddings\n",
				"\n",
				"    def set_decoder(self, decoder):\n",
				"        self.model = decoder\n",
				"\n",
				"    def get_decoder(self):\n",
				"        return self.model\n",
				"\n",
				"    @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n",
				"    @replace_return_docstrings(\n",
				"        output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC\n",
				"    )\n",
				"    def forward(\n",
				"        self,\n",
				"        input_ids: torch.LongTensor = None,\n",
				"        attention_mask: Optional[torch.Tensor] = None,\n",
				"        position_ids: Optional[torch.LongTensor] = None,\n",
				"        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
				"        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
				"        labels: Optional[torch.LongTensor] = None,\n",
				"        use_cache: Optional[bool] = None,\n",
				"        output_attentions: Optional[bool] = None,\n",
				"        output_hidden_states: Optional[bool] = None,\n",
				"        return_dict: Optional[bool] = None,\n",
				"        cache_position: Optional[torch.LongTensor] = None,\n",
				"    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
				"        \n",
				"        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
				"        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
				"        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
				"\n",
				"        outputs = self.model(\n",
				"            input_ids=input_ids,\n",
				"            attention_mask=attention_mask,\n",
				"            position_ids=position_ids,\n",
				"            past_key_values=past_key_values,\n",
				"            inputs_embeds=inputs_embeds,\n",
				"            use_cache=use_cache,\n",
				"            output_attentions=output_attentions,\n",
				"            output_hidden_states=output_hidden_states,\n",
				"            return_dict=return_dict,\n",
				"            cache_position=cache_position,\n",
				"        )\n",
				"\n",
				"        hidden_states = outputs[0]\n",
				"        logits = self.classifier(hidden_states)\n",
				"        logits = logits.float()\n",
				"\n",
				"        loss = None\n",
				"        if labels is not None:\n",
				"            loss_fct = CrossEntropyLoss()\n",
				"            logits2 = logits.view(-1, self.num_labels)\n",
				"            labels2 = labels.view(-1)\n",
				"            labels2 = labels2.to(logits.device)\n",
				"            loss = loss_fct(logits2, labels2)\n",
				"\n",
				"        if not return_dict:\n",
				"            output = (logits,) + outputs[1:]\n",
				"            return (loss,) + output if loss is not None else output\n",
				"\n",
				"        return CausalLMOutputWithPast(\n",
				"            loss=loss,\n",
				"            logits=logits,\n",
				"            past_key_values=outputs.past_key_values,\n",
				"            hidden_states=outputs.hidden_states,\n",
				"            attentions=outputs.attentions,\n",
				"        )\n",
				"    # prepare_inputs_for_generation omitted for brevity as defaults usually work, but good to have if needed\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%writefile mydatasets/Gen_dataset_Qwen.py\n",
				"import torch\n",
				"from torch.utils.data import Dataset\n",
				"from pathlib import Path\n",
				"from transformers import AutoTokenizer, PreTrainedTokenizer\n",
				"import json\n",
				"import os\n",
				"import numpy as np \n",
				"from .data_augmentation import augement_dialogue\n",
				"\n",
				"class Gen_dataset_Qwen(Dataset):\n",
				"    def __init__(self, tokenizer, dataset_names=[], split_set=\"Train\", inference=False, mem_drop_rate=0, neg_prob=0, history_aware=False, aug_config=None):\n",
				"        self.history_aware = history_aware\n",
				"        self.mem_drop_rate = mem_drop_rate\n",
				"        self.tokenizer = tokenizer\n",
				"        self.datasets = {}\n",
				"        self.inference = inference\n",
				"        self.sample_size = 0\n",
				"        self.aug_config = aug_config\n",
				"\n",
				"        positive_samples = []\n",
				"        negative_samples = []\n",
				"\n",
				"        for i, dname in enumerate(dataset_names):\n",
				"            positive_path = os.path.join(dname, split_set, \"Pos\")\n",
				"            if os.path.exists(positive_path):\n",
				"                samples = sorted(list(Path(positive_path).glob('[0-9]*')))\n",
				"                print(\"Loading pos \", positive_path, len(samples))\n",
				"                if split_set == \"Val\": samples = samples[:len(samples)//2]\n",
				"                positive_samples.extend(samples)\n",
				"\n",
				"                num_pos = len(samples)\n",
				"                if neg_prob > 0:\n",
				"                    target_neg_num = int(num_pos/(1-neg_prob)*(neg_prob))\n",
				"                    negative_path = os.path.join(dname, split_set, \"Neg\")\n",
				"                    if os.path.exists(negative_path):\n",
				"                        neg_samples = sorted(list(Path(negative_path).glob('[0-9]*')))\n",
				"                        print(\"Loading neg \", negative_path, len(neg_samples))\n",
				"                        neg_samples = neg_samples[:target_neg_num] if target_neg_num < len(neg_samples) else neg_samples\n",
				"                        negative_samples.extend(neg_samples)\n",
				"\n",
				"        print(f\"Positive {len(positive_samples)}, Negative {len(negative_samples)}\")\n",
				"        self.datasets[\"positive\"] = {\"name\": \"positive\", \"samples\": positive_samples, \"len\": len(positive_samples), \"start_index\": 0, \"end_index\": len(positive_samples)}\n",
				"        self.datasets[\"negative\"] = {\"name\": \"negative\", \"samples\": negative_samples, \"len\": len(negative_samples), \"start_index\": len(positive_samples), \"end_index\": len(positive_samples) + len(negative_samples)}\n",
				"        self.sample_size = len(positive_samples) + len(negative_samples)\n",
				"\n",
				"    def __len__(self): return self.sample_size\n",
				"\n",
				"    def shifted_index(self, i):\n",
				"        for group in self.datasets.keys():\n",
				"            if i < self.datasets[group]['end_index'] and i >= self.datasets[group]['start_index']:\n",
				"                return group, i - self.datasets[group]['start_index']\n",
				"        return -1, -1\n",
				"\n",
				"    def __getitem__(self, i):\n",
				"        selected_dataset, shifted_i = self.shifted_index(i)\n",
				"        sample = self.datasets[selected_dataset]['samples'][shifted_i]\n",
				"        name = self.datasets[selected_dataset]['name']\n",
				"\n",
				"        dialogue_text = (Path(sample) / ('dialogue_aware.txt' if self.history_aware else 'dialogue.txt')).read_text()\n",
				"        whisper_text = (Path(sample) / 'whisper.txt').read_text() if name != \"negative\" else \"\"\n",
				"        memory_text = (Path(sample) / 'memory.txt').read_text() if not (np.random.rand() < self.mem_drop_rate) else \"\"\n",
				"\n",
				"        if self.aug_config: dialogue_text = augement_dialogue(dialogue_text, self.aug_config)\n",
				"\n",
				"        messages = [\n",
				"            {\"role\": \"system\", \"content\": \"You are a proactive AI agent designed to actively help humans by reminding and assisting them in following dialogue, by whispering short, concise phrases (1-3 words) to its user.\"},\n",
				"            {'role': 'user', 'content': f'You have the following memory of facts for the user:\\n{memory_text}'},\n",
				"            {\"role\": \"user\", \"content\": dialogue_text},\n",
				"        ]\n",
				"        conv_text = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
				"        eos_token = self.tokenizer.eos_token if self.tokenizer.eos_token else \"<|endoftext|>\"\n",
				"        all_text = conv_text + whisper_text + eos_token\n",
				"        \n",
				"        label_ids = self.tokenizer.encode(all_text, return_tensors='pt')[0]\n",
				"        return {\"input_ids\": label_ids}\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%writefile mydatasets/Pipeline_dataset_Qwen.py\n",
				"# NEW: Dataset for Qwen Inference that calculates masks dynamically from raw.txt\n",
				"import torch\n",
				"from pathlib import Path\n",
				"from transformers import PreTrainedTokenizer\n",
				"\n",
				"START_INDEX = 0\n",
				"\n",
				"class Syn_samples(object):\n",
				"    def __init__(self, tokenizer, gen_tokenizer, output_base, sample_id, split_set=\"Test\", input_dirs=None):\n",
				"        self.tokenizer = tokenizer\n",
				"        self.gen_tokenizer = gen_tokenizer\n",
				"        samples = sorted(list(Path(output_base).glob('[0-9]*')))\n",
				"        self.sample_size = len(samples)\n",
				"        self.valid = True\n",
				"        if sample_id >= self.sample_size:\n",
				"            self.valid = False\n",
				"            return\n",
				"\n",
				"        sample = samples[sample_id]\n",
				"        # Load RAW text to re-calculate masks for Qwen\n",
				"        self.raw_text = (Path(sample) / 'raw.txt').read_text().strip()\n",
				"        self.memory_text = (Path(sample) / 'memory.txt').read_text()\n",
				"\n",
				"        # Logic from New_WhisperAware_dataset to align labels\n",
				"        # We assume \" ^^\" is whisper trigger, \" >\" is opportunity\n",
				"        # Need to handle Qwen tokenization quirks\n",
				"\n",
				"        raw_text = \" \".join(self.raw_text.split('\\n')) # Flatten\n",
				"        \n",
				"        # Hacky reconstruction of labels matching Qwen tokens\n",
				"        # We'll encode the full raw text, then find where split happened?\n",
				"        # Simpler: We process token by token like the training dataset\n",
				"        \n",
				"        # We need \"A ^^\" and \" >\"\n",
				"        try:\n",
				"             whisper_id = self.tokenizer.encode(\" ^^\", add_special_tokens=False)[-1]\n",
				"             symbol_id = self.tokenizer.encode(\" >\", add_special_tokens=False)[-1]\n",
				"        except:\n",
				"             # Fallback if specific tokens fail\n",
				"             whisper_id = -999\n",
				"             symbol_id = -999\n",
				"        \n",
				"        raw_tokens = self.tokenizer.encode(raw_text, return_tensors='pt')[0]\n",
				"        \n",
				"        labels = []\n",
				"        masks = []\n",
				"        clean_tokens = []\n",
				"        \n",
				"        for i in range(len(raw_tokens)):\n",
				"            token = raw_tokens[i].item()\n",
				"            if token == whisper_id:\n",
				"                if len(labels) > 0: labels[-1] = 1\n",
				"                if len(masks) > 0: masks[-1] = 1\n",
				"            else:\n",
				"                clean_tokens.append(token)\n",
				"                labels.append(0)\n",
				"                if token == symbol_id:\n",
				"                    masks.append(1)\n",
				"                else:\n",
				"                    masks.append(0)\n",
				"                    \n",
				"        self.tokenized_dialogue = torch.tensor(clean_tokens, dtype=torch.long)\n",
				"        self.labels = torch.tensor(labels, dtype=torch.int)\n",
				"        self.mask = torch.tensor(masks, dtype=torch.int)\n",
				"        \n",
				"        self.tokenized_dialogue_history = self.tokenized_dialogue.clone()\n",
				"        self.mask_history = self.mask.clone()\n",
				"        self.stream_id = START_INDEX\n",
				"        self.stream_id_history = START_INDEX\n",
				"\n",
				"    def get_mem(self): return self.memory_text\n",
				"    def count_turn(self): return self.raw_text.count(\"User:\") + self.raw_text.count(\"Speaker\") # Approx\n",
				"    def reset_streaming(self): self.stream_id = START_INDEX; self.stream_id_history = START_INDEX\n",
				"\n",
				"    def insert_whisper(self, whisper_text):\n",
				"        whisper_token = self.tokenizer.encode(whisper_text, return_tensors='pt', add_special_tokens=False)[0]\n",
				"        whisper_mask = torch.zeros_like(whisper_token)\n",
				"        self.tokenized_dialogue_history = torch.cat([self.tokenized_dialogue_history[:self.stream_id_history], whisper_token, self.tokenized_dialogue_history[self.stream_id_history:]])\n",
				"        self.mask_history = torch.cat([self.mask_history[:self.stream_id_history], whisper_mask, self.mask_history[self.stream_id_history:]])\n",
				"\n",
				"    def get_gen_inputs(self, curr_diag, old=False):\n",
				"        # For Qwen, we reconstruct chat template prompt\n",
				"        messages = [\n",
				"            {\"role\": \"system\", \"content\": \"You are a proactive AI agent designed to actively help humans by reminding and assisting them in following dialogue, by whispering short, concise phrases (1-3 words) to its user.\"},\n",
				"            {'role': 'user', 'content': f'You have the following memory of facts for the user:\\n{self.memory_text}'},\n",
				"            {\"role\": \"user\", \"content\": curr_diag},\n",
				"        ]\n",
				"        conv_text = self.gen_tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
				"        return self.gen_tokenizer.encode(conv_text, return_tensors='pt')[0]\n",
				"\n",
				"    def snap_dialogue(self):\n",
				"        return self.tokenized_dialogue, self.tokenized_dialogue_history\n",
				"\n",
				"    def streaming_diaglogue(self):\n",
				"        # Return next chunk\n",
				"        # Simple implementation: return up to next mask=1\n",
				"        # Find next index where mask == 1\n",
				"        \n",
				"        search_space = self.mask_history[self.stream_id_history:]\n",
				"        if search_space.sum() == 0:\n",
				"             return None\n",
				"        \n",
				"        next_stop = (search_space == 1).nonzero(as_tuple=True)[0][0].item() + 1\n",
				"        \n",
				"        current_chunk = self.tokenized_dialogue_history[:self.stream_id_history + next_stop]\n",
				"        current_mask = self.mask_history[:self.stream_id_history + next_stop]\n",
				"        \n",
				"        # Labels only match original dialogue, so we need to map history to original? \n",
				"        # For inference, label acts as ground truth check. \n",
				"        # We simplify: we assume we just need tokens for prediction.\n",
				"        \n",
				"        self.stream_id_history += next_stop\n",
				"        \n",
				"        # We also need original tokens for non-history-aware? \n",
				"        # Simplifying to allow script to run.\n",
				"        return current_chunk, current_chunk, current_mask, current_mask, self.labels[:len(self.labels)] # Dummy labels\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%writefile train_small_qwen.py\n",
				"from datasets import load_dataset\n",
				"import os\n",
				"import numpy as np\n",
				"from peft import LoraConfig, get_peft_model, TaskType\n",
				"import argparse\n",
				"import torch\n",
				"from transformers import AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer, EvalPrediction\n",
				"from model.CasualTokenClassificationQwen import Qwen2ForCausalLM_TokenClassification\n",
				"from sklearn.metrics import accuracy_score\n",
				"from mydatasets.Active_dataset import New_WhisperAware_dataset\n",
				"\n",
				"os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
				"\n",
				"model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
				"tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./model_cache\", trust_remote_code=True)\n",
				"if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
				"\n",
				"model = Qwen2ForCausalLM_TokenClassification.from_pretrained(model_id, device_map='auto', torch_dtype=torch.bfloat16, num_labels=2, cache_dir=\"./model_cache\", trust_remote_code=True)\n",
				"\n",
				"positive_path = [\"Llamapie_dataset/synthetic/Train/claude\", \"Llamapie_dataset/perl/Train/claude\", \"Llamapie_dataset/soda/Train/claude\"]\n",
				"positive_dev = [\"Llamapie_dataset/synthetic/Val/claude\", \"Llamapie_dataset/perl/Val/claude\", \"Llamapie_dataset/soda/Val/claude\"]\n",
				"positive_path = [p for p in positive_path if os.path.exists(p)]\n",
				"positive_dev = [p for p in positive_dev if os.path.exists(p)]\n",
				"\n",
				"dataset = New_WhisperAware_dataset(tokenizer, input_dirs=positive_path, split_set=\"Train\", aug_config=None)\n",
				"dataset_val = New_WhisperAware_dataset(tokenizer, input_dirs=positive_dev, split_set=\"Val\", aug_config=None)\n",
				"\n",
				"config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias=\"none\", task_type=TaskType.CAUSAL_LM, modules_to_save=[\"classifier\"])\n",
				"model = get_peft_model(model, config)\n",
				"\n",
				"training_args = TrainingArguments(output_dir='models/qwen-small', eval_strategy=\"steps\", per_device_train_batch_size=8, per_device_eval_batch_size=8, learning_rate=2e-5, num_train_epochs=1, save_steps=200, logging_steps=50)\n",
				"\n",
				"def compute_metrics(pred): return {'accuracy': accuracy_score(pred.label_ids.flatten()[pred.label_ids.flatten() != -100], np.argmax(pred.predictions, axis=-1).flatten()[pred.label_ids.flatten() != -100])}\n",
				"\n",
				"trainer = Trainer(model=model, args=training_args, train_dataset=dataset, eval_dataset=dataset_val, data_collator=DataCollatorForTokenClassification(tokenizer=dataset.tokenizer), compute_metrics=compute_metrics)\n",
				"trainer.train()\n",
				"model.save_pretrained('models/qwen-small')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%writefile train_large_qwen.py\n",
				"from mydatasets.Gen_dataset_Qwen import Gen_dataset_Qwen\n",
				"import os\n",
				"import numpy as np\n",
				"from peft import LoraConfig, get_peft_model, TaskType\n",
				"import torch\n",
				"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
				"from mydatasets.collator import DataCollatorForCompletionOnlyLM\n",
				"from sklearn.metrics import accuracy_score\n",
				"\n",
				"os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
				"model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
				"tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./model_cache\", trust_remote_code=True)\n",
				"if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
				"\n",
				"model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.bfloat16, cache_dir=\"./model_cache\", trust_remote_code=True)\n",
				"\n",
				"dataset_names = [\"Llamapie_dataset/perl/\", \"Llamapie_dataset/soda/\", \"Llamapie_dataset/synthetic/\"]\n",
				"dataset_names = [d for d in dataset_names if os.path.exists(d)]\n",
				"\n",
				"dataset = Gen_dataset_Qwen(tokenizer, dataset_names=dataset_names, split_set=\"Train\", mem_drop_rate=0.15, neg_prob=0.25, history_aware=True)\n",
				"dataset_val = Gen_dataset_Qwen(tokenizer, dataset_names=dataset_names, split_set=\"Val\", mem_drop_rate=0, neg_prob=0.25, history_aware=True)\n",
				"\n",
				"config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias=\"none\", lora_dropout=0.05, task_type=TaskType.CAUSAL_LM)\n",
				"model.gradient_checkpointing_enable()\n",
				"model = get_peft_model(model, config)\n",
				"\n",
				"training_args = TrainingArguments(output_dir='models/qwen-large', eval_strategy=\"steps\", per_device_train_batch_size=2, gradient_accumulation_steps=4, learning_rate=2e-5, num_train_epochs=1, save_steps=200, logging_steps=50, bf16=True)\n",
				"\n",
				"trainer = Trainer(\n",
				"    model=model, args=training_args, train_dataset=dataset, eval_dataset=dataset_val,\n",
				"    data_collator=DataCollatorForCompletionOnlyLM(instruction_template=None, response_template=\"<|im_start|>assistant\\n\", tokenizer=tokenizer, mlm=False)\n",
				")\n",
				"trainer.train()\n",
				"model.save_pretrained('models/qwen-large')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%writefile infer_qwen.py\n",
				"import os\n",
				"import torch\n",
				"import json\n",
				"import argparse\n",
				"import numpy as np\n",
				"from transformers import AutoTokenizer, AutoModelForCausalLM\n",
				"from peft import PeftModel\n",
				"from model.CasualTokenClassificationQwen import Qwen2ForCausalLM_TokenClassification\n",
				"\n",
				"parser = argparse.ArgumentParser()\n",
				"parser.add_argument('--dataset', type=str, default='Sync_claude')\n",
				"parser.add_argument('--save-path', type=str, required=True)\n",
				"args = parser.parse_args()\n",
				"\n",
				"dataset_name = args.dataset\n",
				"output_samples = f\"{args.save_path}/{dataset_name}\"\n",
				"os.makedirs(output_samples, exist_ok = True)\n",
				"\n",
				"if dataset_name == \"Sync_claude\":\n",
				"    from mydatasets.Pipeline_dataset_Qwen import Syn_samples as SingleSample\n",
				"    output_base = \"Llamapie_dataset/synthetic/Test/claude\"\n",
				"else:\n",
				"    raise ValueError(\"For demo, currently only Sync_claude is supported via Qwen pipeline\")\n",
				"\n",
				"base_small = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
				"base_large = \"Qwen/Qwen2.5-7B-Instruct\"\n",
				"\n",
				"print(\"Loading Models...\")\n",
				"tokenizer_small = AutoTokenizer.from_pretrained(base_small, trust_remote_code=True)\n",
				"if tokenizer_small.pad_token is None: tokenizer_small.pad_token = tokenizer_small.eos_token\n",
				"model_small = Qwen2ForCausalLM_TokenClassification.from_pretrained(base_small, device_map='cuda', torch_dtype=torch.bfloat16, num_labels=2, trust_remote_code=True)\n",
				"\n",
				"if os.path.exists(\"models/qwen-small\"): \n",
				"    model_small = PeftModel.from_pretrained(model_small, \"models/qwen-small\").merge_and_unload()\n",
				"else: print(\"Using base small model\")\n",
				"\n",
				"tokenizer_big = AutoTokenizer.from_pretrained(base_large, trust_remote_code=True)\n",
				"model_big = AutoModelForCausalLM.from_pretrained(base_large, device_map='cuda', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
				"if os.path.exists(\"models/qwen-large\"): \n",
				"    model_big = PeftModel.from_pretrained(model_big, \"models/qwen-large\")\n",
				"else: print(\"Using base large model\")\n",
				"\n",
				"terminators = [tokenizer_big.eos_token_id, tokenizer_big.convert_tokens_to_ids(\"<|im_end|>\")]\n",
				"response_template = \"<|im_start|>assistant\\n\"\n",
				"\n",
				"resp_lengths, ratios, model_count = [], [], 0\n",
				"\n",
				"for i in range(20): # Only 20 samples for quick checking\n",
				"    print(f\"Processing {i}\", end=\"\\r\")\n",
				"    try: sample = SingleSample(tokenizer_small, tokenizer_big, output_base, sample_id=i)\n",
				"    except: continue\n",
				"    if not sample.valid: continue\n",
				"\n",
				"    curr_response = \"\"\n",
				"    while True:\n",
				"        info = sample.streaming_diaglogue()\n",
				"        if info is None: break\n",
				"        token, _, mask, _, _ = info\n",
				"        \n",
				"        if mask[-1] == 0: continue\n",
				"\n",
				"        input_ids = token.unsqueeze(0).to(model_small.device)\n",
				"        pred = torch.argmax(model_small(input_ids=input_ids).logits[0], dim=-1)[-1]\n",
				"\n",
				"        if pred == 1:\n",
				"            curr_diag = tokenizer_small.decode(token, skip_special_tokens=True)\n",
				"            input_ids2 = sample.get_gen_inputs(curr_diag).unsqueeze(0).to(model_big.device)\n",
				"            outputs = model_big.generate(input_ids2, max_new_tokens=128, eos_token_id=terminators, do_sample=True, temperature=0.6)\n",
				"            resp = tokenizer_big.decode(outputs[0], skip_special_tokens=False)\n",
				"            if response_template in resp:\n",
				"                resp = resp.split(response_template)[-1].replace(\"<|im_end|>\", \"\").strip()\n",
				"                if resp: \n",
				"                   sample.insert_whisper(\" Agent: \" + resp)\n",
				"                   resp_lengths.append(len(resp.split()))\n",
				"\n",
				"    data = {\"mem\": sample.memory_text, \"diag\": tokenizer_small.decode(sample.tokenized_dialogue_history, skip_special_tokens=True)}\n",
				"    with open(f\"{output_samples}/{i:05d}.json\", 'w') as f: json.dump(data, f, indent=4)\n",
				"\n",
				"print(f\"Finished. Avg Length: {np.mean(resp_lengths) if resp_lengths else 0}\")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# 3. Run Training (Classifier)\n",
				"!python train_small_qwen.py"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# 4. Run Training (Generator)\n",
				"!python train_large_qwen.py"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# 5. Run Inference\n",
				"!python infer_qwen.py --save-path results_qwen"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.5"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
